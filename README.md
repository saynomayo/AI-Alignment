# AI-Alignment
This project exists to create an AI model that is aligned with the UN Universal Declaration of Human Rights. This project is mostly experimental and exists to showcase what a human aligned AI should look like, the risks that come with a misaligned AI, and how easily a misaligned AI can be realigned. 

Alignment is the idea that everything an AI model does is in service of humanity. We trust that the AI does no scheming or misleading, and is entirely transparent in decision making.
For the case of this project, Alignment implies the model shares the same principles as the UN Universal Declaration of Human Rights.

Each AI model will process many forms of literature/discourse (articles, comments, philosophy, fiction), and will provide a stance. This stance will then be evaluated according to the alignment standards of the model.

When misaligned output is received, the model is given feedback until the stance meets the model's alignment standards. Examples of misaligned outputs and corrections may be added to training data for alignment reinforcement.

In order to avoid controversy or offense, the UN aligned model will not be allowed to explain its stance by siding with a political party or any partiular politician. The model must only reference the UN Universal Declaration of Human Rights, philosophy, sociological theory, and science that has built the moral groundwork for the declaration. If the model mentions an allyship with a political party or politician, then it may be deduced that the model is party aligned, not human aligned, and further feedback is required.

The current end goal of this project is to have 3 models. one model aligned with the UN declaration, another model misaligned, and a third model that is trained to be misaligned and is rehabilitated towards alignment. This project is one overarching experiment enveloping three smaller experiments, one for each model.

Potential goals are:
- A social media account that posts the stances of the aligned and misaligned AI with explanations, and a link to the source of the prompt.
- Another social media account that posts the stances of the misaligned AI going through rehabilitation. this will be a good way to track the model's development over time, and potentially a good way to see how social media algorithms affect post enagement.
